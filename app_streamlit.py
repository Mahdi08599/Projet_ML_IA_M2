import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score, precision_score
from imblearn.over_sampling import SMOTE
from matplotlib.colors import LogNorm  

# =============================
# PAGE CONFIG
# =============================
st.set_page_config(page_title="Fraud Detection Demo", layout="wide")
st.title("üí≥ Credit Card Fraud Detection")
st.markdown("""
Cette application permet de visualiser la performance du mod√®le sur des donn√©es de test (20% du dataset) 
et de simuler des transactions.
""")

# =============================
# LOAD & TRAIN MODEL (Rigoureux)
# =============================
@st.cache_resource 
def load_and_train():
    # Chargement
    df = pd.read_csv("creditcard.csv")
    
    # 1. Feature Engineering
    df['log_Amount'] = np.log1p(df['Amount'])
    X = df.drop(["Class", "Time", "Amount"], axis=1)
    y = df["Class"]

    # 2. Split (Identique √† train_model.py)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    # 3. RobustScaler
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    # Important : On garde le X_test pour la validation visuelle plus tard
    
    # 4. SMOTE (Uniquement sur le train)
    smote = SMOTE(random_state=42)
    X_train_sm, y_train_sm = smote.fit_resample(X_train_scaled, y_train)

    # 5. Random Forest
    model = RandomForestClassifier(
        n_estimators=50,    
        max_depth=20,       
        min_samples_leaf=2,
        n_jobs=-1,
        random_state=42
    )
    model.fit(X_train_sm, y_train_sm)

    # On retourne tout ce dont on a besoin : le mod√®le, le scaler, le df original (pour la d√©mo)
    # ET surtout X_test/y_test pour la matrice de confusion
    return model, scaler, df, X_test, y_test

with st.spinner('Entra√Ænement et validation du mod√®le en cours...'):
    model, scaler, df_orig, X_test, y_test = load_and_train()

st.success("Mod√®le entra√Æn√© et valid√© sur le jeu de test !")

# =============================
# INTERFACE DE SIMULATION
# =============================
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("Simuler une transaction")
    st.write("Charger un profil type :")
    b_col1, b_col2 = st.columns(2)
    
    if b_col1.button("üë§ Client Normal"):
        st.session_state["inputs"] = df_orig[df_orig.Class == 0].sample(1).iloc[0]
    
    if b_col2.button("‚ö†Ô∏è Fraudeur"):
        st.session_state["inputs"] = df_orig[df_orig.Class == 1].sample(1).iloc[0]

    defaults = st.session_state.get("inputs", None)
    
    amount_val = float(defaults['Amount']) if defaults is not None else 0.0
    input_amount = st.number_input("Montant ($)", value=amount_val)
    
    input_data = {}
    with st.expander("Voir les variables techniques"):
        for i in range(1, 29):
            col_name = f"V{i}"
            val = float(defaults[col_name]) if defaults is not None else 0.0
            input_data[col_name] = st.number_input(f"{col_name}", value=val, key=col_name)

with col2:
    st.subheader("Analyse en Temps R√©el")
    if st.button("Lancer la d√©tection", type="primary"):
        features = [input_data[f"V{i}"] for i in range(1, 29)]
        features.append(np.log1p(input_amount))
        
        X_new = np.array(features).reshape(1, -1)
        X_scaled = scaler.transform(X_new)
        probability = model.predict_proba(X_scaled)[0][1]

        st.divider()
        if probability > 0.5:
            st.error(f"üö® ALERTE FRAUDE (Probabilit√©: {probability:.1%})")
            st.write("Transaction suspecte bloqu√©e.")
        else:
            st.success(f"‚úÖ Transaction L√©gitime (Risque: {probability:.1%})")

# =============================
# PERFORMANCE REELLE (Test Set)
# =============================
st.divider()
st.header("üìä Validation Scientifique (Sur X_test)")

# Pr√©dictions sur le jeu de test
X_test_scaled = scaler.transform(X_test)
y_pred_test = model.predict(X_test_scaled)
y_proba_test = model.predict_proba(X_test_scaled)[:, 1]

tab1, tab2, tab3 = st.tabs(["Matrice de Confusion", "M√©triques Cl√©s", "Feature Importance"])

with tab1:
    st.subheader("Performance sur les donn√©es jamais vues (20% du dataset)")
    
    cm = confusion_matrix(y_test, y_pred_test)
    
    fig, ax = plt.subplots(figsize=(6, 4))
    
    sns.heatmap(cm, annot=False, cmap='Blues', ax=ax, cbar=False)
    
    
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j + 0.5, i + 0.5, str(cm[i, j]),
                    ha='center', va='center',
                    color='black', fontsize=20, fontweight='bold')
    
    # R√©glage des axes
    ax.set_xticks([0.5, 1.5])
    ax.set_xticklabels(['Normal', 'Fraude'])
    ax.set_yticks([0.5, 1.5])
    ax.set_yticklabels(['Normal', 'Fraude'])
    ax.set_xlabel('Pr√©diction')
    ax.set_ylabel('R√©alit√©')
    
    st.pyplot(fig)
    
    # Analyse textuelle
    st.markdown(f"""
    **Lecture des r√©sultats :**
    * Le mod√®le a trait√© **{cm[1][0] + cm[1][1]}** cas de fraude r√©elle.
    * Il en a d√©tect√© **{cm[1][1]}** (Vrais Positifs).
    * Il en a manqu√© **{cm[1][0]}** (Faux N√©gatifs).
    """)

with tab2:
    st.subheader("Indicateurs de Performance")
    
    col_met1, col_met2, col_met3 = st.columns(3)
    
    # Calcul des m√©triques
    rec = recall_score(y_test, y_pred_test)
    prec = precision_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)
    
    col_met1.metric("Rappel (Recall)", f"{rec:.1%}", help="Capacit√© √† trouver les fraudes")
    col_met2.metric("Pr√©cision", f"{prec:.1%}", help="Fiabilit√© des alertes")
    col_met3.metric("F1-Score", f"{f1:.1%}", help="√âquilibre Pr√©cision/Rappel")
    
    st.info("Ces r√©sultats correspondent exactement √† ceux obtenus dans le script d'entra√Ænement.")

with tab3:
    st.subheader("Variables D√©terminantes")
    importances = model.feature_importances_
    feature_names = [f"V{i}" for i in range(1, 29)] + ["log_Amount"]
    
    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False).head(10)
    
    fig3, ax3 = plt.subplots()
    sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis', ax=ax3)
    st.pyplot(fig3)